from opencage.geocoder import OpenCageGeocode
import polyline
import requests
import streamlit as st
import pandas as pd
import json
import base64
from github import Github, GithubException
from io import BytesIO


@st.cache_data
def charger_donnees(nom_fichier="data/hebergements_chemins.parquet", format=None, branche="main"):
    """
    Fonction pour charger des donn√©es depuis un d√©p√¥t GitHub priv√©.

    Args:
        nom_fichier: Chemin du fichier relatif √† la racine du d√©p√¥t
        format: Format de conversion souhait√©
        branche: Nom de la branche (par d√©faut: "main")
    """
    try:
        # R√©cup√©rer les identifiants depuis les secrets Streamlit
        token = st.secrets["github"]["token"]
        repo_name = st.secrets["github"]["repo_name"]

        # Initialiser le client GitHub
        g = Github(token)
        repo = g.get_repo(repo_name)

        try:
            # R√©cup√©rer le contenu du fichier √† partir de la branche sp√©cifi√©e
            contents = repo.get_contents(nom_fichier, ref=branche)

            # D√©coder le contenu du fichier
            decoded_content = base64.b64decode(contents.content)

            # Cr√©er un objet BytesIO pour lire le contenu
            buffer = BytesIO(decoded_content)

            # Convertir selon le format demand√©
            if format == 'parquet':
                return pd.read_parquet(buffer)
            else:
                buffer.seek(0)
                return buffer

        except Exception as e:
            st.warning(f"Erreur lors de l'acc√®s au fichier {nom_fichier} sur la branche {branche}: {e}")
            return None

    except Exception as e:
        st.error(f"Erreur lors de l'acc√®s au d√©p√¥t GitHub: {e}")
        return None


def sauvegarder_donnees(contenu, nom_fichier, message_commit="Mise √† jour des donn√©es", branche="main"):
    """
    Fonction pour sauvegarder des donn√©es dans un d√©p√¥t GitHub priv√© sans cr√©er de copie locale.

    Args:
        contenu: Contenu √† sauvegarder (DataFrame, dict, str, bytes, ou BytesIO)
        nom_fichier: Nom du fichier √† sauvegarder
        message_commit: Message pour le commit GitHub
        branche: Nom de la branche (par d√©faut: "main")

    Returns:
        bool: True si la sauvegarde a r√©ussi, False sinon
    """
    try:
        # R√©cup√©rer les identifiants depuis les secrets Streamlit
        token = st.secrets["github"]["token"]
        repo_name = st.secrets["github"]["repo_name"]

        # Convertir le contenu en bytes selon son type
        if isinstance(contenu, pd.DataFrame):
            # Pour un DataFrame pandas
            buffer = BytesIO()
            if nom_fichier.endswith('.parquet'):
                contenu.to_parquet(buffer, index=False)
            elif nom_fichier.endswith('.csv'):
                contenu.to_csv(buffer, index=False)
            else:
                contenu.to_csv(buffer, index=False)  # CSV par d√©faut
            buffer.seek(0)
            github_content = buffer.read()

        elif isinstance(contenu, dict) or isinstance(contenu, list):
            # Pour un dictionnaire ou une liste (format JSON)
            github_content = json.dumps(contenu).encode('utf-8')

        elif isinstance(contenu, str):
            # Pour une cha√Æne de caract√®res
            github_content = contenu.encode('utf-8')

        elif isinstance(contenu, bytes):
            # Pour des donn√©es binaires
            github_content = contenu

        elif isinstance(contenu, BytesIO):
            # Pour un BytesIO
            contenu.seek(0)
            github_content = contenu.read()

        else:
            st.error(f"Type de contenu non pris en charge: {type(contenu)}")
            return False

        # Initialiser le client GitHub
        g = Github(token)
        repo = g.get_repo(repo_name)

        try:
            # V√©rifier si le fichier existe d√©j√†
            contents = repo.get_contents(nom_fichier, ref=branche)
            # Mettre √† jour le fichier existant
            repo.update_file(
                path=contents.path,
                message=message_commit,
                content=github_content,
                sha=contents.sha,
                branch=branche
            )
        except GithubException as e:
            if e.status == 404:
                # Si le fichier n'existe pas, le cr√©er
                repo.create_file(
                    path=nom_fichier,
                    message=message_commit,
                    content=github_content,
                    branch=branche
                )
            else:
                raise e

        # Invalider le cache pour forcer un rechargement des donn√©es
        if 'charger_donnees' in globals() and hasattr(charger_donnees, 'clear'):
            charger_donnees.clear()

        st.success(f"‚úÖ Fichier {nom_fichier} sauvegard√© sur GitHub")
        return True

    except Exception as e:
        st.error(f"Erreur lors de la sauvegarde sur GitHub: {e}")
        import traceback
        st.error(traceback.format_exc())
        return False


def add_lat_lon(df, address_column="Adresse"):
    """Ajoute les coordonn√©es g√©ographiques (latitude, longitude) pour chaque adresse"""
    try:
        api_key = st.secrets["opencage"]["api_key"]
    except KeyError:
        st.error("Cl√© API OpenCage manquante. V√©rifiez le fichier .streamlit/secrets.toml.")
        return df

    geocoder = OpenCageGeocode(api_key)

    # Cr√©er les colonnes Latitude et Longitude si elles n'existent pas
    if "Latitude" not in df.columns:
        df["Latitude"] = None
    if "Longitude" not in df.columns:
        df["Longitude"] = None

    def get_coordinates(address):
        try:
            result = geocoder.geocode(address)
            if result:
                return result[0]["geometry"]["lat"], result[0]["geometry"]["lng"]
        except Exception as e:
            st.error(f"Erreur pour {address} : {e}")
        return None, None

    for index, row in df.iterrows():
        if pd.isna(row["Latitude"]) or pd.isna(row["Longitude"]):
            lat, lon = get_coordinates(row[address_column])
            df.at[index, "Latitude"] = lat
            df.at[index, "Longitude"] = lon

    st.info("‚úÖ Latitude et Longitude ajout√©es avec succ√®s !")
    return df


def get_osrm_route(lat1, lon1, lat2, lon2):
    """Interroge OSRM pour obtenir le trac√© et la distance entre deux points."""
    # V√©rifier que les coordonn√©es sont valides
    if None in (lat1, lon1, lat2, lon2):
        print("Coordonn√©es invalides, impossible de calculer l'itin√©raire.")
        return None, None

    url = (
        f"http://router.project-osrm.org/route/v1/driving/"
        f"{lon1},{lat1};{lon2},{lat2}"
        "?overview=full&geometries=polyline"
    )
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        if data.get("routes"):
            route = data["routes"][0]
            distance_km = route["distance"] / 1000  # Conversion en km
            route_coords = polyline.decode(route["geometry"])  # Liste de (lat, lon)
            return distance_km, json.dumps(route_coords)  # Sauvegarde en JSON
    return None, None


def calculate_routes_osrm(df):
    """Calcule les distances et les trajets avec OSRM si non enregistr√©s."""
    # Cr√©er une copie du DataFrame pour √©viter de modifier l'original
    df = df.copy()

    # S'assurer que les colonnes n√©cessaires existent
    for col in ["Latitude", "Longitude"]:
        if col not in df.columns:
            df[col] = None

    if "Chemin" not in df.columns:
        df["Chemin"] = None
    if "Distance (km)" not in df.columns:
        df["Distance (km)"] = None

    # V√©rifier s'il y a des coordonn√©es manquantes et les ajouter
    missing_coords = df[df["Latitude"].isna() | df["Longitude"].isna()].index
    if len(missing_coords) > 0:
        # Appliquer add_lat_lon seulement aux lignes avec coordonn√©es manquantes
        df_missing = df.loc[missing_coords].copy()
        df_missing = add_lat_lon(df_missing)

        # Mettre √† jour le DataFrame original avec les nouvelles coordonn√©es
        for idx in missing_coords:
            df.loc[idx, "Latitude"] = df_missing.loc[idx, "Latitude"]
            df.loc[idx, "Longitude"] = df_missing.loc[idx, "Longitude"]

    # Initialiser les listes pour stocker les r√©sultats
    distances = []
    route_geoms = []

    # Calculer les itin√©raires pour chaque segment
    for i in range(len(df) - 1):  # On parcourt jusqu'√† l'avant-dernier point
        lat1, lon1 = df.iloc[i]["Latitude"], df.iloc[i]["Longitude"]
        lat2, lon2 = df.iloc[i + 1]["Latitude"], df.iloc[i + 1]["Longitude"]

        # V√©rifier si toutes les coordonn√©es sont valides
        valid_coords = pd.notna(lat1) and pd.notna(lon1) and pd.notna(lat2) and pd.notna(lon2)

        # Si les coordonn√©es sont valides et le trac√© est d√©j√† calcul√©, on le r√©cup√®re
        if valid_coords and pd.notna(df.iloc[i]["Chemin"]) and pd.notna(df.iloc[i]["Distance (km)"]):
            try:
                route_coords = df.iloc[i]["Chemin"]
                if isinstance(route_coords, str):
                    route_coords = json.loads(route_coords)
                distance = df.iloc[i]["Distance (km)"]
            except Exception as e:
                st.warning(f"Erreur lors de la lecture du chemin √† l'index {i}: {e}")
                distance, route_coords = get_osrm_route(lat1, lon1, lat2, lon2)
        # Sinon, on calcule un nouveau trac√© si les coordonn√©es sont valides
        elif valid_coords:
            distance, route_coords = get_osrm_route(lat1, lon1, lat2, lon2)
        # Si les coordonn√©es sont invalides, on ne peut pas calculer de trac√©
        else:
            st.warning(f"Coordonn√©es manquantes pour le segment {i} √† {i + 1}, impossible de calculer l'itin√©raire.")
            distance = None
            route_coords = json.dumps([])

        # Mettre √† jour le DataFrame directement
        df.at[i, "Distance (km)"] = distance

        # S'assurer que route_coords est au format JSON
        if isinstance(route_coords, list):
            route_coords = json.dumps(route_coords)
        df.at[i, "Chemin"] = route_coords

        # Stocker pour retour de fonction
        distances.append(distance)
        route_geoms.append(route_coords)

    # Ajouter une derni√®re valeur pour correspondre √† la taille du DataFrame
    distances.append(None)
    route_geoms.append(json.dumps([]))

    return distances, route_geoms, df


def identifier_sejours_multiples(df):
    """Identifie les s√©jours multiples au m√™me endroit et met √† jour les dur√©es"""
    # Cr√©er une copie pour √©viter de modifier le DataFrame original
    df_avec_duree = df.copy()

    # Initialiser les colonnes pour la dur√©e du s√©jour et la date de fin
    df_avec_duree['Duree_Sejour'] = 1
    df_avec_duree['Date_Fin'] = None

    # Premi√®re passe pour identifier les groupes de s√©jour
    groupes_sejour = []
    groupe_actuel = [0]  # Commencer avec la premi√®re ligne

    for i in range(1, len(df_avec_duree)):
        # Comparer avec le dernier √©l√©ment du groupe actuel
        dernier_index = groupe_actuel[-1]

        # V√©rifier si l'adresse actuelle est la m√™me que celle du dernier √©l√©ment du groupe
        meme_adresse = df_avec_duree.iloc[dernier_index]['Adresse'] == df_avec_duree.iloc[i]['Adresse']
        memes_coords = (df_avec_duree.iloc[dernier_index]['Latitude'] == df_avec_duree.iloc[i]['Latitude'] and
                        df_avec_duree.iloc[dernier_index]['Longitude'] == df_avec_duree.iloc[i]['Longitude'])

        if meme_adresse or memes_coords:
            # M√™me endroit, ajouter √† ce groupe
            groupe_actuel.append(i)
        else:
            # Nouvel endroit, terminer le groupe actuel et en commencer un nouveau
            groupes_sejour.append(groupe_actuel)
            groupe_actuel = [i]

    # Ajouter le dernier groupe
    groupes_sejour.append(groupe_actuel)

    # Deuxi√®me passe pour mettre √† jour les dur√©es et dates
    for groupe in groupes_sejour:
        if len(groupe) > 1:  # S√©jour multiple
            premier_index = groupe[0]
            dernier_index = groupe[-1]

            # Mettre √† jour le premier √©l√©ment du groupe
            df_avec_duree.at[premier_index, 'Duree_Sejour'] = len(groupe)
            df_avec_duree.at[premier_index, 'Date_Fin'] = df_avec_duree.iloc[dernier_index]['Nuit']

            # Marquer les autres √©l√©ments du groupe comme √† fusionner
            for i in groupe[1:]:
                df_avec_duree.at[i, 'Duree_Sejour'] = -1

    return df_avec_duree


def ouvrir_pdf(chemin_pdf, use_expander=False):
    """
    Version minimale pour afficher un PDF dans Streamlit

    Args:
        chemin_pdf: Chemin du fichier PDF √† charger
        use_expander: Utiliser un expander pour afficher le PDF
    """
    try:
        import os

        # Charger le fichier PDF depuis GitHub
        contenu_pdf = charger_donnees(nom_fichier=chemin_pdf, format="binary")

        if not contenu_pdf:
            st.error("Impossible de charger le fichier PDF.")
            return

        # Extraire le nom du fichier du chemin
        nom_fichier = os.path.basename(chemin_pdf)

        # Pr√©parer les donn√©es binaires
        if hasattr(contenu_pdf, 'read'):
            contenu_pdf.seek(0)
            pdf_data = contenu_pdf.read()
        else:
            pdf_data = contenu_pdf

        # Fonction pour l'affichage du contenu
        def afficher_contenu():
            # Titre et bouton de t√©l√©chargement
            st.subheader(f"üìÑ {nom_fichier}")

            # Solution de repli simple avec iframe
            import base64
            b64_pdf = base64.b64encode(pdf_data).decode('utf-8')
            pdf_display = f'<iframe src="data:application/pdf;base64,{b64_pdf}" width="100%" height="1000" type="application/pdf"></iframe>'
            st.markdown(pdf_display, unsafe_allow_html=True)

        # Afficher avec ou sans expander
        if use_expander:
            with st.expander(f"Document: {nom_fichier}", expanded=True):
                afficher_contenu()
        else:
            afficher_contenu()

    except Exception as e:
        st.error(f"Erreur lors de l'ouverture du PDF: {e}")
        import traceback
        st.error(traceback.format_exc())